0720 수업 내용

1. anaconda 설치 
: python 기본 toolkit + 외부 라이브러리 
: 관리자 권한으로 실행
: path 우선 순위 변경 
pip
2. EDitor - jupyter notebook
: Command line interface(CLI) : Python idle >>>
: ipython 을 기반으로 해서 브라우저상에서 사용하는 에디터 
: 크롬을 기본 브라우저로 설정해야

1. pandas : table data(표데이터) 처리
데이터 분석, db에 저장 , 시각화 연동
https://dataitgirls2.github.io/10minutes2pandas/
2. requests :  http client(통신)
https://requests.readthedocs.io/en/master/ 
3. beatifulsoup : html, xml 데이터 parsing(뽑아내)
https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 
4. matplotlib : visualization(시각화), 폰트 , 막대그래프...  
https://pypi.org/project/matplotlib/
5. seaborn : visualization(시각화)
https://github.com/mwaskom/seaborn
https://seaborn.pydata.org/index.html#seaborn-statistical-data-visualization
6. pymysql : mysql db connector
https://github.com/PyMySQL/PyMySQL/
7. pymongo
https://github.com/mongodb/mongo-python-driver

8. sqlalchemy :  
ORM(Object Relation Mapping) 
Sql 에서만 쓰임
Tablular data : dataframe 객체
이걸 쓰면 자동으로 rdb 매핑, 객체만 만들면 됨, 속성 등등 이런거 미리 안해도 됨   `
Orm rule
Class <=> table
Object <=> row(record) 행 , 우리는 표데이터만 만들면 됨
Variable <=> column 백넘버, 선수 이름, 선수 키………..

9. jupyter notebook/ jupyter lab
https://jupyter-notebook.readthedocs.io/en/stable/
https://pypi.org/ 원하는 오픈소스 다운 
https://requests.readthedocs.io/en/master/ requests http
https://www.crummy.com/software/BeautifulSoup/bs4/doc/ beautifulsoup

오늘은 requests, beatifulsoup
월 14-15, 화16-17 책 읽기


181페이지
Pickle : 통신 같은거 할때 객체를 파일로 저장하고 다시 불러올 때 사용 가능
   Dump : 저장
   Load : 불러오기

*******************
Parising : 구문
1. 정규표현식(regular expression) 사용
2. beautifulsoup 사용 
주로 2번 사용// 1번이 어려움// 가끔 1번 사용

Version --python
path
Pip show beautifulsoup4
Pip show 


***************************

*******주피터 노트북 띄우기
C:\Users\yujin> cd C:\Python\webscrap_source    
>>Jupyter notebook - 내가 하고자 하는 파일 바로 나와

# markdown 셀타입 폰트 달라져

-크롬 - 도구 - 개발자도구 f12
________________________________________________________________
web scraping과 데이터 분석 
1. naver 뉴스 제목 검색
2. naver api : 파파고 번역 서비스
3. naver webtoon 다운로드 & 업로드 해보기
- 1~3 : requests, 
4. 기상청에서 제공하는 날씨데이터 검색
- 4 : beautifulsoup
5. pandas 연습
     : 행정구역 정보 csv 데이터 
     : pandas, seaborn, matplotlib
<<6, 7, 이 하이라이트
6. 멜론 노래 차트 100
7. 국회의원 정보 검색
     : requests, beautifulsoup, pandas, seaborn
matplotlib, pymysql(mariadb), sqlalchemy 
8. 팟빵 사이트 
     : mp4파일 다운로드...
9. mongodb
     : cine21 영화나 영화배우 정보 검색

10. 머신러닝 ..!!!
11. 그다음 장고 웹 프레임워크 
**이번주 디비 연동하여 오픈소스 배우는 시간
**다음주 8,9 이런 시간들..................


get : 데이터 조회, url 에 append해서 보내는 방식
post : body stream에 데이터 담아서 보내는 방식 , 등록 갱신

클라이언트 -> 서버 : 요청
               <-         응답  response


___________________________________________
url ='https://news.naver.com/'
response = requests.get(url)
response.status_code

>>200 
200: http 코드 정상이라는 뜻 

cookie 
session
- http 프로토콜은 connectionless 
- 쿠키나 세션으로 요청을 보낸 곳/응답 주는 곳 서로 구분하기 위함
- 사용자 정보 커넥션 끊어지니까 여기에 임시 저장 하는 역할

css selector
<div id = "myDiv"> //고유 아이디 값으로 태그를 객체로 접근하여 핸들링 가능
<ul>
<div class = "myClass">
    <li> aaa</li>
</ul> 
<a href = "www.naver.com"></a>
<img = src="">

soup.select('li')   // 괄호안에 태그명
1)tag selector
: tag명으로 특정 tag을 선택
2) ID selector #
soup.select('#myDiv')
id가 div인 것을 선택해!!
3) class selector
soup.select('div.myClass')
4)attribute selector []
soup.select("a[href ='www.naver.com' ]")
*= : 부분 매칭 
^= : 시작
$= : 종료


https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=011&aid=0003771063
#today_main_news > div.hdline_news > ul > li:nth-child(1) > div.hdline_article_tit > a
https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=011&aid=0003771063
https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=011&aid=0003771063





###1.nhn 뉴스 제목 검색

requests, beautifulsoup 사용
css selector - 속성 선택자 사용


import requests
from bs4 import BeautifulSoup

url ='https://news.naver.com/'
response = requests.get(url)
print('status_code:', response.status_code)
print('response header : ', response.headers)
response.headers['content-type']

status_code: 200
response header :  {'date': 'Mon, 20 Jul 2020 06:24:12 GMT', 'cache-control': 'no-cache', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'set-cookie': 'JSESSIONID=2F6AEE107BEBEBB5BEF7CD8F48352996; Path=/main; HttpOnly', 'content-language': 'ko-KR', 'vary': 'Accept-Encoding', 'content-encoding': 'gzip', 'transfer-encoding': 'chunked', 'content-type': 'text/html;charset=EUC-KR', 'referrer-policy': 'unsafe-url', 'server': 'nfront'}
'text/html;charset=EUC-KR'

response.text #그래서 parsing 쓴다 이렇게 보기 어려우니까 


#beautiful Soup 생성
soup = BeautifulSoup(html, 'html.parser')
tag_list = soup.select("a[href*=read.nhn]")
print(type(tag_list), len(tag_list))
